#import relevant libraries:
# TensorFlow and tf.keras
import tensorflow as tf
import os
from sklearn.model_selection import train_test_split #splits dataset into testing and training

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

#import data from Google Drive (credit: https://stackoverflow.com/questions/65907971/downloading-data-from-google-drive-colab)
import pathlib
from google.colab import drive
drive.mount('/content/gdrive')
data_dir = "/content/gdrive/My Drive/ObjectDetection/Images_Combined"

#make category names
category_names = ['Plastic bottles', 'Metal sorted', 'Glass sorted', 'cardboard']
train_labels = np.array(category_names)
#split images into training and testing datasets
import os
from sklearn.model_selection import train_test_split #splits dataset into testing and training (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
image_files = os.listdir(data_dir)

# Filter out only the image files with .jpg extension using a for loop
jpg_image_files = []
for file_name in image_files:
    if file_name.endswith('.jpg'):
        jpg_image_files.append(file_name)

# Split the filtered image files into training and testing sets
train_files, test_files = train_test_split(jpg_image_files, test_size= 0.2, random_state= 50)

#pre-processing images for standardization and noise reduction on the model:
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def preprocess_image(file_path):
    img = load_img(file_path, target_size=(128, 128))
    img_array = img_to_array(img) / 255.0  # Convert image to numpy array and normalize
    return img_array

# Preprocess training images and create corresponding labels
train_data = []
train_labels = []

for file_name in train_files:
    img_array = preprocess_image(os.path.join(data_dir, file_name))
    train_data.append(img_array)
    category_index = train_files.index(file_name)
    label = category_names[category_index % len(category_names)]  # Ensure circular index
    train_labels.append(label)

# Preprocess testing images and create corresponding labels
test_data = []
test_labels = []

for file_name in test_files:
    img_array = preprocess_image(os.path.join(data_dir, file_name))
    test_data.append(img_array)
    category_index = test_files.index(file_name)
    label = category_names[category_index % len(category_names)]  # Ensure circular index
    test_labels.append(label)

# Convert lists to numpy arrays
train_data = np.array(train_data)
train_labels = np.array(train_labels)

test_data = np.array(test_data)
test_labels = np.array(test_labels)

#Model Time!!!
from tensorflow.keras import layers, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode='nearest'

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.5),  # Dropout layer with dropout rate of 0.5
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(4, activation='softmax')  # Output layer with 4 units
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy
              metrics=['accuracy'])

from sklearn.preprocessing import LabelEncoder

# Encode the training labels
label_encoder = LabelEncoder()
train_labels_encoded = label_encoder.fit_transform(train_labels)

# Feed the model with training data
model.fit(train_datagen.flow(train_data, train_labels_encoded, batch_size=32),
                    epochs=20,
                    validation_data=(test_data, test_labels_encoded))

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(test_data, test_labels_encoded)
print('Test accuracy:', test_acc)
